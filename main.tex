\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}

\graphicspath{ {./} }


\title{A Comparative Analysis of Machine Learning Models for Predicting the Lorenz System}
\author{Rushi Chaudhary \\
Memorial University of Newfoundland \\
rrchaudhary@mun.ca}
\date{October 27, 2025}

\begin{document}

\maketitle

\section{Introduction}

The Lorenz system, introduced by Edward N. Lorenz in his seminal 1963 paper on deterministic nonperiodic flow, serves as a foundational model in the study of chaotic dynamics\cite{lorenz1963deterministic}. Originally derived as a simplified mathematical representation of atmospheric convection, the system consists of three coupled nonlinear ordinary differential equations:

\begin{align}
\dot{x} &= \sigma (y - x), \\
\dot{y} &= x (\rho - z) - y, \\
\dot{z} &= xy - \beta z,
\end{align}

where $x$, $y$, and $z$ represent variables related to convective motion, horizontal temperature variation, and vertical temperature variation, respectively. The parameters $\sigma$, $\rho$, and $\beta$ control the system's behavior. The Lorenz system exhibits chaotic attractors characterized by sensitive dependence on initial conditions—a phenomenon popularly known as the ``butterfly effect''\cite{gleick1987chaos}. This sensitivity implies that small changes in starting points can lead to vastly different trajectories, making long-term prediction inherently challenging despite the system's deterministic nature.

Chaotic systems like the Lorenz model are ubiquitous in natural phenomena, appearing in fields such as meteorology, fluid dynamics, and even biology. Traditional numerical methods for solving these ordinary differential equations (ODEs), such as Euler's method or higher-order Runge-Kutta integrators, provide reliable short-term forecasts but accumulate errors over time due to numerical instability and approximations. As a result, predicting the long-term behavior of such systems requires innovative approaches that can capture underlying patterns while mitigating error propagation.

In recent decades, machine learning (ML) has emerged as a powerful tool for modeling and forecasting complex dynamical systems. Unlike traditional methods that rely on explicit integration of known equations, ML techniques can learn directly from data, identifying hidden structures in time-series observations. For chaotic systems, recurrent neural networks (RNNs), particularly Long Short-Term Memory (LSTM) architectures, have shown promise in handling sequential data and capturing temporal dependencies\cite{dubois2020data}. These models excel in short-term predictions by learning from historical trajectories but may struggle with long-term stability due to the amplification of small errors in chaotic regimes.

To address these limitations, physics-informed approaches integrate domain knowledge into ML frameworks. Physics-Informed Neural Networks (PINNs), for instance, embed the governing differential equations into the loss function, ensuring that predictions adhere to physical laws while fitting the data. Related scientific ML methods, such as Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs), model the derivative functions themselves using neural networks, offering interpretable and efficient alternatives for simulating ODE systems\cite{kashyap2024modeling}. These hybrid techniques combine the flexibility of data-driven learning with the robustness of physical constraints, potentially yielding superior performance in chaotic environments.

This project undertakes a comparative analysis of various ML models—regression, LSTMs, and PINNs—for predicting the evolution of the Lorenz system. The primary goal is to assess their efficacy in both short-term and long-term forecasting horizons, quantified through metrics such as mean squared error (MSE) and root mean squared error (RMSE). Additionally, we aim to explain the theoretical underpinnings of their performances

\includegraphics[scale=0.4]{img.png}

\section{Methods}
\label{sec:methods}

\subsection{Problem formulation}
We consider the Lorenz system
\begin{align}
\dot{x} &= \sigma (y - x), \\
\dot{y} &= x (\rho - z) - y, \\
\dot{z} &= x y - \beta z,
\end{align}
where $(x,y,z)$ denote the state variables and $(\sigma,\rho,\beta)$ are constant parameters. The learning task is one-step-ahead prediction: given a recent history of states, predict the next state $(x_{t+\Delta t}, y_{t+\Delta t}, z_{t+\Delta t})$.

\subsection{Data generation and preprocessing}
Two trajectories were generated by numerically integrating the Lorenz system using a variable–step solver and then resampling to a uniform time grid using interpolation to obtain evenly spaced samples:
\begin{itemize}
  \item Training/validation trajectory: 10{,}000 samples from initial condition $[1,\,1,\,1]$.
  \item Test trajectory (held out): 10{,}000 samples from initial condition $[5,\,10,\,15]$.
\end{itemize}
The state vectors were then standardized (zero mean and unit variance), and the same transformation was applied to validation and test sets. Generating separate test and training data sets ensures that we're testing true generalization, not memorization.

\subsection{Supervised framing}
We construct rolling-window sequences of fixed length $L=50$ to predict the next time step. For a standardized time series $\{\mathbf{s}_t\}_{t=1}^{N}$ with $\mathbf{s}_t \in \mathbb{R}^3$, define input--target pairs
\begin{equation}
\mathbf{X}_t = \big[\mathbf{s}_{t-L+1},\,\ldots,\,\mathbf{s}_{t}\big] \in \mathbb{R}^{L \times 3}, 
\qquad
\mathbf{y}_t = \mathbf{s}_{t+1} \in \mathbb{R}^{3},
\end{equation}
for $t=L,\ldots,N-1$. This yields $N-L$ sequences per split.

\subsection{Models}
We compare three approaches that take $\mathbf{X}_t$ as input and predict $\widehat{\mathbf{y}}_t$.

\vspace{5mm}

\textbf{\normalsize Baseline regression (Ridge):}  
Flatten the last $L$ time steps ($L\times 3$ features) into a single vector and apply a linear model with $\ell_2$ regularization:
\[
\widehat{\mathbf{y}}_t = W\,\mathrm{vec}(\mathbf{X}_t) + \mathbf{b}, 
\quad
\min_{W,\mathbf{b}}\;\frac{1}{M}\sum_{t}\|\widehat{\mathbf{y}}_t - \mathbf{y}_t\|_2^2 + \lambda \|W\|_F^2.
\]

\textbf{Expected behavior:} 
\begin{itemize}
  \item Captures some nonlinearity via high-dimensional features but remains a smooth linear map in those features and extrapolates poorly.
  \item Fast and robust baseline for one-step prediction; typically outperformed by sequence models.

\end{itemize}

\textbf{\normalsize Long Short-Term Memory (LSTM):}  
A recurrent neural network that ingests the $L\times 3$ sequence in order, compresses it into a hidden state, and maps that to the next state via a dense layer. Trained with MSE using Adam and early stopping.

\textbf{Expected behavior:}
\begin{itemize}
  \item Learns nonlinear temporal dependencies leading to better performances on time series.
  \item Performance depends on sequence length $L$, hidden size, regularization, and the match between training and test distributions.
\end{itemize}

\textbf{\normalsize Physics-Informed Neural Network (PINN):}
 A neural network $f_\theta(t)=(x_\theta,y_\theta,z_\theta)$ trained to both fit observed data and satisfy the Lorenz ODE via automatic differentiation. The loss combines data fidelity and physics residuals:
\begin{align}
\mathcal{L}_{\text{data}} &= \frac{1}{N_d}\sum_{i} \big\| f_\theta(t_i) - \mathbf{s}(t_i) \big\|_2^2, \\
\mathcal{L}_{\text{phys}} &= \frac{1}{N_c}\sum_{j} \Big(
\|\dot{x}_\theta - \sigma(y_\theta - x_\theta)\|_2^2 +
\|\dot{y}_\theta - (x_\theta(\rho - z_\theta) - y_\theta)\|_2^2 +
\|\dot{z}_\theta - (x_\theta y_\theta - \beta z_\theta)\|_2^2 \Big), \\
\mathcal{L} &= \mathcal{L}_{\text{data}} + \lambda_{\text{phys}}\,\mathcal{L}_{\text{phys}}.
\end{align}
\textbf{Expected behavior:}
\begin{itemize}
  \item Encodes correct physics, improving data efficiency and physical consistency. 
  \item May not minimize one-step MSE as effectively as LSTM but long Term fitting outclasses LSTM with careful tuning.
\end{itemize}

\subsection{Training, validation, and testing}
From the standardized trajectory with initial condition $[1,1,1]$, the first $3000$ samples form the training series and the next $7000$ samples form the validation series. The test set is the separate standardized trajectory (10{,}000 samples) generated from $[5,10,15]$ and is never used for model fitting or selection.

\subsection{Evaluation}
One-step-ahead mean squared error (MSE) on the test set:
\begin{equation}
\mathrm{MSE} = \frac{1}{M}\sum_{t=1}^{M} \big\| \widehat{\mathbf{y}}_t - \mathbf{y}_t \big\|_2^2,
\end{equation}
computed for windows of length $L=50$.

\section{Results}
\label{sec:results}

\subsection{One-step-ahead accuracy}
We evaluated one-step-ahead prediction on the held-out Lorenz trajectory generated from initial condition $[5,10,15]$ using a rolling window of length $L=50$. Mean squared error (MSE) was computed in standardized coordinates over the test sequences ($9950$ windows).

\begin{center}
\begin{tabular}{l c}
\hline
Model & Test MSE \\
\hline
Random Forest & 0.063599 \\
LSTM & 0.000663 \\
PINN & 0.029885 \\
\hline
\end{tabular}
\end{center}

\includegraphics[scale=0.3]{image.png}


\subsection{Unexpected outcome relative to hypothesis}
We hypothesized in the previous section that a physics-informed model (PINN), which enforces the Lorenz ODE through residual penalties, would outperform a purely data-driven sequence model (LSTM) data due to its stronger inductive bias and improved data efficiency. Contrary to this expectation, the LSTM achieved the lowest one-step MSE, while the PINN performed intermediate between the LSTM and the Random Forest.

\subsection{Possible explanations}
The observed ranking (LSTM $<$ PINN $<$ Random Forest in MSE) is consistent with several plausible factors:
\begin{itemize}
  \item \textbf{Different training goal vs.\ evaluation score.} The PINN is trained to be \emph{physics-consistent} \cite{raissi2019physics}, not to minimize the next-step error directly. Our test metric is one-step MSE, which favors models (like the LSTM) trained exactly for that target.
  \item \textbf{Overfitting vs.\ regularization.} The LSTM likely had just enough capacity and regularization to fit short-term patterns without overfitting noise. The PINN can \emph{underfit} locally when the physics penalty is too strong or the network is small, and the Random Forest can \emph{overfit} local patterns yet still miss smooth dynamics. Recent studies document optimization and failure modes in PINNs. These studies emphasize the difficuly in optimising the loss functions in PINNs  \cite{krishnapriyan2021failure}

\end{itemize}

\bibliographystyle{plain}
\bibliography{bibfile}

\end{document}