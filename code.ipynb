{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5051a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dea0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated TRAINING trajectory: 10000 points, initial=[1.0, 1.0, 1.0]\n",
      "Generated TEST trajectory: 10000 points, initial=[5.0, 10.0, 15.0]\n",
      "\n",
      "Training samples: 3000\n",
      "Validation samples: 7000\n",
      "Test samples: 10000 (from separate trajectory)\n",
      "Training sequences: 2950, Val: 6950, Test: 9950\n",
      "\n",
      "[1/3] Training Random Forest Regression...\n",
      "[2/3] Training LSTM...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. GENERATE LORENZ SYSTEM DATA\n",
    "# ============================================================================\n",
    "\n",
    "def lorenz(t, state, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    x, y, z = state\n",
    "    return [sigma*(y-x), x*(rho-z)-y, x*y-beta*z]\n",
    "\n",
    "# Training data: original trajectory\n",
    "# We generate our training data using initial conditions [1, 1, 1]\n",
    "# spanning 50 time units with 10,000 data points.\n",
    "t_span, n_points = (0, 50), 10000\n",
    "sol_train = solve_ivp(lorenz, t_span, [1.0, 1.0, 1.0], \n",
    "                      t_eval=np.linspace(*t_span, n_points), method='RK45')\n",
    "t_train, trajectory_train = sol_train.t, sol_train.y.T\n",
    "\n",
    "# Importantly, we generate a SEPARATE test trajectory with completely \n",
    "# different initial conditions [5, 10, 15]. This ensures we're testing true \n",
    "# generalization, not memorization.\"\n",
    "sol_test = solve_ivp(lorenz, t_span, [5.0, 10.0, 15.0], \n",
    "                     t_eval=np.linspace(*t_span, n_points), method='RK45')\n",
    "t_test_full, trajectory_test = sol_test.t, sol_test.y.T\n",
    "\n",
    "print(f\"Generated TRAINING trajectory: {len(t_train)} points, initial=[1.0, 1.0, 1.0]\")\n",
    "print(f\"Generated TEST trajectory: {len(trajectory_test)} points, initial=[5.0, 10.0, 15.0]\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. PREPARE DATA\n",
    "# ============================================================================\n",
    "# We split our training data into 30% for training and 70% for \n",
    "# validation. The test set comes entirely from our separate trajectory\n",
    "n_train = int(0.3*len(trajectory_train))\n",
    "train_data = trajectory_train[:n_train]\n",
    "val_data = trajectory_train[n_train:]\n",
    "\n",
    "# Test data: use the separate trajectory\n",
    "test_data = trajectory_test\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)} (from separate trajectory)\")\n",
    "\n",
    "# Standardize : Converts to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_data)\n",
    "val_scaled = scaler.transform(val_data)\n",
    "test_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Create sequences (use 50 timesteps to predict next state)\n",
    "def create_sequences(data, seq_len=50):\n",
    "    X = [data[i:i+seq_len] for i in range(len(data)-seq_len)]\n",
    "    y = [data[i+seq_len] for i in range(len(data)-seq_len)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_sequences(train_scaled)\n",
    "X_val, y_val = create_sequences(val_scaled)\n",
    "X_test, y_test = create_sequences(test_scaled)\n",
    "print(f\"Training sequences: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TRAIN MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Random Forest Regression\n",
    "# Our first model is Random Forest\n",
    "# It's a traditional ML approach that doesn't understand temporal dependencies\n",
    "# but can capture complex non-linear relationships\n",
    "print(\"\\n[1/3] Training Random Forest Regression...\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest = RandomForestRegressor(n_estimators=10, random_state=42, n_jobs=-1)\n",
    "forest.fit(X_train.reshape(len(X_train), -1), y_train)\n",
    "forest_pred = forest.predict(X_test.reshape(len(X_test), -1))\n",
    "forest_mse = mean_squared_error(y_test, forest_pred)\n",
    "\n",
    "# LSTM (Long Short Term Memory)\n",
    "# Our second model is an LSTM - a type of recurrent neural network.\n",
    "# designed specifically for sequential data. It has memory cells that can \n",
    "# learn temporal dependencies and patterns over time.\n",
    "print(\"[2/3] Training LSTM...\")\n",
    "lstm = keras.Sequential([\n",
    "    layers.LSTM(64, input_shape=(50, 3)),\n",
    "    layers.Dense(3)\n",
    "])\n",
    "lstm.compile(optimizer='adam', loss='mse')\n",
    "lstm.fit(X_train, y_train, epochs=50, batch_size=32, \n",
    "         validation_data=(X_val, y_val), verbose=0)\n",
    "lstm_pred = lstm.predict(X_test, verbose=0)\n",
    "lstm_mse = mean_squared_error(y_test, lstm_pred)\n",
    "\n",
    "# PINN (Physics-Informed Neural Network)\n",
    "# Our third model is a PINN - it's unique because it incorporates \n",
    "# the actual Lorenz equations into the training process. This physics-informed\n",
    "# approach constrains the network to respect the underlying physical laws.\"\n",
    "print(\"[3/3] Training PINN...\")\n",
    "class PINN(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d1 = layers.Dense(32, activation='tanh')\n",
    "        self.d2 = layers.Dense(32, activation='tanh')\n",
    "        self.out = layers.Dense(3)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.out(self.d2(self.d1(x)))\n",
    "\n",
    "pinn = PINN()\n",
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "dt = t_train[1] - t_train[0] \n",
    "\n",
    "for epoch in range(50):\n",
    "    for i in range(0, len(X_train), 32):\n",
    "        batch_X, batch_y = X_train[i:i+32], y_train[i:i+32]\n",
    "        state = batch_X[:, -1, :]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = pinn(state, training=True)\n",
    "            data_loss = tf.reduce_mean(tf.square(pred - batch_y))\n",
    "            \n",
    "            # Physics loss (Lorenz equations)\n",
    "            x, y, z = state[:, 0], state[:, 1], state[:, 2]\n",
    "            x_p, y_p, z_p = pred[:, 0], pred[:, 1], pred[:, 2]\n",
    "            dx = (x_p - x)/dt - 10*(y - x)\n",
    "            dy = (y_p - y)/dt - (x*(28 - z) - y)\n",
    "            dz = (z_p - z)/dt - (x*y - 8/3*z)\n",
    "            physics_loss = tf.reduce_mean(tf.square(dx) + tf.square(dy) + tf.square(dz))\n",
    "            \n",
    "            loss = data_loss + 0.1 * physics_loss\n",
    "        \n",
    "        grads = tape.gradient(loss, pinn.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, pinn.trainable_variables))\n",
    "\n",
    "pinn_pred = pinn(X_test[:, -1, :], training=False).numpy()\n",
    "pinn_mse = mean_squared_error(y_test, pinn_pred)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. RESULTS\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESULTS - Test Set MSE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Random Forest:  {forest_mse:.6f}\")\n",
    "print(f\"LSTM:              {lstm_mse:.6f}\")\n",
    "print(f\"PINN:              {pinn_mse:.6f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. VISUALIZATION\n",
    "# ============================================================================\n",
    "# Inverse transform predictions\n",
    "y_test_orig = scaler.inverse_transform(y_test)\n",
    "forest_pred_orig = scaler.inverse_transform(forest_pred)\n",
    "lstm_pred_orig = scaler.inverse_transform(lstm_pred)\n",
    "pinn_pred_orig = scaler.inverse_transform(pinn_pred)\n",
    "\n",
    "# Get test time points (from the test trajectory)\n",
    "t_test = t_test_full[50:50+len(y_test)]  \n",
    "\n",
    "# Create interpolated smooth curves\n",
    "t_interp = np.linspace(t_test[0], t_test[-1], 1000)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "models = [('Random Forest', forest_pred_orig), ('LSTM', lstm_pred_orig), ('PINN', pinn_pred_orig)]\n",
    "coords = ['X', 'Y', 'Z']\n",
    "\n",
    "for row, (model_name, pred) in enumerate(models):\n",
    "    for col, coord in enumerate(coords):\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Interpolate\n",
    "        f_actual = interp1d(t_test, y_test_orig[:, col], kind='cubic')\n",
    "        f_pred = interp1d(t_test, pred[:, col], kind='cubic')\n",
    "        \n",
    "        actual_interp = f_actual(t_interp)\n",
    "        pred_interp = f_pred(t_interp)\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(t_interp, actual_interp, 'b-', label='Actual', linewidth=1.5, alpha=0.7)\n",
    "        ax.plot(t_interp, pred_interp, 'r--', label='Predicted', linewidth=1.5, alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Time', fontsize=10)\n",
    "        ax.set_ylabel(f'{coord} value', fontsize=10)\n",
    "        ax.set_title(f'{model_name} - {coord} component', fontsize=11, fontweight='bold')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
